---
title: "FRTCI: Systematic Variation Estimation Package Tutorial"
author: "Ding, P., Feller, A., and Miratrix, L."
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction
This document demonstrates how to use the systematic variation estimation methods of the `FRTCI` package. First load the package:
```{r}
library( FRTCI )
```

This package has code to make synthetic data.  Here we make a dataset with 3 covariates that we will use to illustrate the function calls.  For this dataset, the first two variables have a systematic relationship with treatment impact, and the third is good for adjustment for increasing power:
```{r}
df = make.randomized.dat( 100, gamma.vec=c(1,1,1,2), beta.vec=c(-1,-1,1) )
str( df )
```

## Basic estimation
The function `est.beta` is our core estimator (the naive estimator):
```{r}
est.beta( Yobs, Z,  ~ A + B, data=df )
```
The arguments are observed outcome, observed treatment assignment, and what variables to find a systematic relationship to, expressed as a formula using the tilde notation (categorical covariates will be automatically converted).

The names of the variables don't matter:
```{r}
df2 = df[c("A","B","Yobs","Z") ]
names(df2) = c( "AA", "BB", "myY", "myZ" )
rs = est.beta( myY, myZ, myY ~ AA + BB, data=df2 )
rs$beta.hat
```


We can obtain our standard errors and variance-covariance matrix that comes from the design-based theory:
```{r}
vcov( rs )
SE( rs )
```
and confidence intervals (using the normal approximation)
```{r}
confint( rs )
```

# OLS adjustment

OLS is basically using the emperical covariance matrix Sxx.hat for each treatment arm rather than the known Sxx:
```{r}
M.ols.ours = est.beta( Yobs, Z, ~ A + B, data=df, method="OLS" )
M.ols.ours
M.ols.ours$beta.hat
```

Simple interaction-based OLS approach, to compare with:
```{r}
M0 = lm( Yobs ~ (A+B) * Z, data=df )
M0

M.ols.ours$beta - coef(M0)[4:6]
```
There are no differences up to machine precision.


# Model adjustment

The model-adjusted estimator automatically is used if you give
two formula, one for the treatment model, one for the control
 adjustment model

```{r}
est.beta( Yobs, Z, ~ A + B, ~ A + B + C, data=df )
```

# Model adjustment + OLS adjustment

using adjustment: we can do with and without emperical Sxx. 
```{r}
est.beta( Yobs, Z,  ~ A + B, ~ A + B + C, data=df )$beta.hat
est.beta( Yobs, Z,  ~ A + B, ~ A + B + C, data=df, empirical.Sxx = TRUE )$beta.hat
```
same as model-adjustment + empirical.Sxx, above
```{r}
est.beta( Yobs, Z,  ~ A + B, ~ A + B + C, data=df, method="OLS" )$beta.hat
```

# Oracle estimator (for simulations and verification of formulae)
If we know all potential outcomes, we can calculate the exact beta for the sample
We can also get the true SE, which is why we pass a sample treatment vector (so it can calculate proportion treated, under the assumption of simple random assignment):  
```{r}
Moracle = calc.beta.oracle( Y.1, Y.0,  Z, ~ A + B, data=df )
Moracle
SE( Moracle )
```



# When there is no het tx variation 

Here we see all the methods on a scenario with no variation
```{r}
df = make.randomized.dat( 1000, beta.vec=c(-1,0,0) )

calc.beta.oracle( Y.1, Y.0, Z, ~ A + B, data=df )$beta.hat
est.beta( Yobs, Z, ~ A + B, data=df )$beta.hat
est.beta( Yobs, Z, ~ A + B, data=df, method="OLS" )$beta.hat
```



# Looking at R^2

We can look at treatment effect explained.  We will look at two scenarios, one with no ideosyncratic variation, and one with a substantial amount.  We will plot the R2 sensitivity curves for each on top of each other.
```{r}
df = make.randomized.dat( 1000, beta.vec=c(-1,1,1) )
rs = est.beta( Yobs, Z, ~ A + B, data=df, method="OLS" )
r2 = R2.ITT( rs )
r2    
plot( r2 )
df = make.randomized.dat( 1000, beta.vec=c(-1,1,1), ideo.sd=3 )
rs = est.beta( Yobs, Z, ~ A + B, data=df, method="OLS" )
r2 = R2.ITT( rs )
r2    
plot( r2, ADD=TRUE, col="green" )
```

And here is a case where we have 100% systematic variation along a single variable.
```{r}
df = make.randomized.dat( 1000, beta.vec=c(-1,1,0) )
rs = est.beta( Yobs, Z, ~ A + B, data=df, method="OLS" )
r2 = R2.ITT( rs )
r2    
plot( r2 )
```

See, we have 100% R2, if we knew the true individual treatment effects:
```{r}
plot( df$tau ~ df$A )
```

## Comparing estimators
Here we look at how results differ across different estimators
```{r}
df = make.randomized.dat( 1000, beta.vec=c(-1,1,1), ideo.sd=1 )

rs = est.beta( Yobs, Z, ~ A + B, data=df )
r2 = R2.ITT( rs )
r2    
plot( r2, col="green" )

# adjusted
rs = est.beta( Yobs, Z, ~ A + B, ~ A + B, data=df )
r2 = R2.ITT( rs )
r2    
plot( r2, ADD=TRUE )

# adjusted + OLS
rs = est.beta( Yobs, Z, ~ A + B, ~ A + B + C, data=df, empirical.Sxx = TRUE )
r2 = R2.ITT( rs )
r2    
plot( r2, ADD=TRUE, col="blue" )
```



# Testing LATE estimators

Our estimators also work in non-compliance contexts (see paper for details).  The story is analogous to the code above.

For this illustration we generate some fake data using a provided function included with the package.  It takes a complier treatment heterogeniety model defined by `beta`:
```{r}
beta = c(-1,6,0)
n = 1000

data = make.randomized.compliance.dat( n, beta.vec=beta )
names(data)
head( data )
zd = with( data, interaction( Z, D, sep="-" ) )
```

Our four observable groups defined by treatment assignment and take-up are as follows:
```{r observed_subgroups}
boxplot( Yobs ~ zd, data=data )
```

The true relationships for the three latent groups are as follows:
```{r}
par( mfrow=c(1,2), mgp=c(1.8,0.8,0), mar=c(3,3,0.5,0.5) )
plot( Y.1 - Y.0 ~ A, data=data, col=as.factor(data$S), pch=19, cex=0.5 )
plot( Y.1 - Y.0 ~ B, data=data, col=as.factor(data$S), pch=19, cex=0.5 )
legend( "topleft", legend=levels( as.factor( data$S ) ), pch=19, col=1:3 )
```

(We see no impacts for the AT and the NT as required under the assumptions of noncompliance here.)

## Estimating the effects

```{r}
rs = est.beta.LATE( Z, D, Yobs, ~ A + B, data=data )
rs
rs$beta.hat
SE( rs )

# error
rs$beta.hat - beta

r2 = R2.LATE( rs )
r2
plot( r2 )
```

## The 2SLS Approach
```{r}
rs = est.beta.LATE( Z, D, Yobs,  ~ A + B, data=data, method="2SLS" )
rs
SE( rs )
rs$beta.hat
rs$beta.hat - beta
```

